{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "12F6qa3Z3Fst"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "import warnings\n",
    "import os\n",
    "import keras\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "YzjNM3xI3Fs0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-adabound\r\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/83/81b9e73aa089b0646feaacf911cd22cc4d5ea0374ef03609c35004b025e7/keras-adabound-0.4.1.tar.gz\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from keras-adabound) (1.16.3)\r\n",
      "Requirement already satisfied: Keras in /opt/conda/lib/python3.6/site-packages (from keras-adabound) (2.2.4)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-adabound) (1.1.0)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-adabound) (1.12.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras->keras-adabound) (3.12)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-adabound) (1.0.7)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from Keras->keras-adabound) (1.0.9)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras->keras-adabound) (2.9.0)\r\n",
      "Building wheels for collected packages: keras-adabound\r\n",
      "  Building wheel for keras-adabound (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Stored in directory: /tmp/.cache/pip/wheels/26/fb/78/f6aa020cb8f098fecdf1e9043a9bb259c8414692d4225c6183\r\n",
      "Successfully built keras-adabound\r\n",
      "Installing collected packages: keras-adabound\r\n",
      "Successfully installed keras-adabound-0.4.1\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n",
      "Collecting segmentation-models\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/91/c727ce9f5465d3e4546f69a8ce52446bf34ab4970b5b056582a49cd4a471/segmentation_models-0.2.0-py2.py3-none-any.whl (45kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.3MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from segmentation-models) (0.15.0)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from segmentation-models) (1.0.7)\r\n",
      "Collecting image-classifiers==0.2.0 (from segmentation-models)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/32/a1e74e03f74506d1e4b46bb2732ca5a7b18ac52a36b5e3547e63537ce74c/image_classifiers-0.2.0-py2.py3-none-any.whl (76kB)\r\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 8.3MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: keras>=2.2.0 in /opt/conda/lib/python3.6/site-packages (from segmentation-models) (2.2.4)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->segmentation-models) (2.1)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->segmentation-models) (3.0.3)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->segmentation-models) (0.5.2)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->segmentation-models) (5.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->segmentation-models) (1.1.0)\r\n",
      "Requirement already satisfied: imageio>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from scikit-image->segmentation-models) (2.3.0)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.7->segmentation-models) (2.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.7->segmentation-models) (1.16.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (3.12)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (1.12.0)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras>=2.2.0->segmentation-models) (1.0.9)\r\n",
      "Requirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->segmentation-models) (4.3.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (2.6.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (2.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (1.0.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->segmentation-models) (39.1.0)\r\n",
      "Installing collected packages: image-classifiers, segmentation-models\r\n",
      "Successfully installed image-classifiers-0.2.0 segmentation-models-0.2.0\r\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-adabound\n",
    "!pip install segmentation-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yceA-X653Fs-"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VY39qdVJ3Fs_"
   },
   "outputs": [],
   "source": [
    "train_path = '../input/train_v2/'\n",
    "test_path = '../input/test_v2/'\n",
    "data = pd.read_csv('../input/train_ship_segmentations_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lWvDysu3FtB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.cm import get_cmap\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "import gc; gc.enable() # memory is tight\n",
    "\n",
    "train_image_dir = train_path\n",
    "test_image_dir = test_path\n",
    "\n",
    "def multi_rle_encode(img, **kwargs):\n",
    "    '''\n",
    "    Encode connected regions as separated masks\n",
    "    '''\n",
    "    labels = label(img)\n",
    "    if img.ndim > 2:\n",
    "        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n",
    "    else:\n",
    "        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img, min_max_threshold=1e-3, max_mean_threshold=None):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    if np.max(img) < min_max_threshold:\n",
    "        return '' ## no need to encode if it's all zeros\n",
    "    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n",
    "        return '' ## ignore overfilled mask\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    if type(in_mask_list) == 'float':\n",
    "        return all_masks\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return all_masks\n",
    "\n",
    "def masks_as_color(in_mask_list):\n",
    "    # Take the individual ship masks and create a color mask array for each ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.float)\n",
    "    scale = lambda x: (len(in_mask_list)+x+1) / (len(in_mask_list)*2) ## scale the heatmap image to shift \n",
    "    for i,mask in enumerate(in_mask_list):\n",
    "        if isinstance(mask, str):\n",
    "            all_masks[:,:] += scale(i) * rle_decode(mask)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWJeVULN3FtD"
   },
   "source": [
    "## Data Generator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-nRkffc3FtD",
    "outputId": "7d1f5d22-3e9e-469f-e2cf-4cbb253bd39e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py:5434: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120013, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "val = data[~msk]\n",
    "\n",
    "train_no_ship = train[train[\"EncodedPixels\"].isnull()]\n",
    "train_with_ship = train[train[\"EncodedPixels\"].notnull()]\n",
    "train_with_ship['ship_count'] = train_with_ship.groupby('ImageId')['ImageId'].transform('count')\n",
    "train_with_ship['ship_count'].fillna(0,inplace=True) \n",
    "train_no_ship['ship_count'] = 0\n",
    "print(train_no_ship.shape)\n",
    "sns.distplot(train_with_ship['ship_count'],kde=False);\n",
    "\n",
    "down_sampled_no_ship = train_no_ship.sample(2000)\n",
    "balanced_train = pd.concat([down_sampled_no_ship, train_with_ship])\n",
    "#balanced_train = train_with_ship\n",
    "sns.distplot(balanced_train['ship_count'],kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39i4aqZM3FtG"
   },
   "outputs": [],
   "source": [
    "partition = {}\n",
    "partition['train'] = balanced_train['ImageId'].values\n",
    "partition['val'] = val['ImageId'].values\n",
    "#labels = dict(zip(data['ImageId'].values, data['EncodedPixels'].values)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZar7dxP3FtI"
   },
   "outputs": [],
   "source": [
    "from skimage.transform import rotate\n",
    "from skimage import exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZcRlfbp3FtJ"
   },
   "outputs": [],
   "source": [
    "def transform(image, mask, rate=0.15):\n",
    "    rnd = np.random.uniform(0, 1, 5)\n",
    "    ## contrast\n",
    "    if rnd[2] < rate:\n",
    "        v_min, v_max = np.percentile(image, (0.2, 99.8))\n",
    "        image= exposure.rescale_intensity(image, in_range=(v_min, v_max))\n",
    "        \n",
    "    ## horizontal flip\n",
    "    if rnd[3] < rate:\n",
    "        image= image[:, ::-1]\n",
    "        mask = mask[:, ::-1]\n",
    "    \n",
    "    ## vertical flip\n",
    "    if rnd[4] < rate:\n",
    "        image= image[::-1, :]\n",
    "        mask = mask[::-1, :]\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    ## rotation\n",
    "    if rnd[0] < rate:\n",
    "        deg = np.random.choice(range(6, 25))\n",
    "        image= rotate(image, deg)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def rotation(image):\n",
    "    deg = np.random.choice(range(6, 25))\n",
    "    image= rotate(image, deg)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tH9MgHVv3FtK"
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4K437Fp3FtL"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, data, batch_size=5, dim=(512, 512, 3), shuffle=True, resize_img = False):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.list_IDs = list_IDs\n",
    "        self.shuffle = shuffle\n",
    "        self.resize = resize_img\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        gc.collect()\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size, self.dim[0], self.dim[1], 1), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            path = os.path.join(train_path, ID)\n",
    "            image = imread(path)\n",
    "            encoded_pixels = self.data[self.data['ImageId']==ID]['EncodedPixels']\n",
    "            mask = masks_as_image(encoded_pixels)\n",
    "            if self.resize:\n",
    "                resized_x = cv2.resize(image, (self.dim[0], self.dim[1]))\n",
    "                resized_y = cv2.resize(mask, (self.dim[0], self.dim[1]))\n",
    "                reshaped_y = np.reshape(resized_y, (self.dim[0], self.dim[1], 1))\n",
    "                tx, y[i] = transform(resized_x, reshaped_y)\n",
    "                X[i,] = tx/255.0\n",
    "                \n",
    "            else:\n",
    "                reshaped_y = np.reshape(mask, (self.dim[0], self.dim[1], 1))\n",
    "                tx, y[i] = transform(image, reshaped_y)\n",
    "                X[i,] = tx/255.0\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auBvF68G3FtT"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "params = {'dim': (512, 512, 3),\n",
    "          'batch_size': 8,\n",
    "          'shuffle': True, \n",
    "         'resize_img':True}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "params = {'dim': (768, 512, 3),\n",
    "          'batch_size': 1,\n",
    "          'shuffle': True}\n",
    "\"\"\"\n",
    "# Generators\n",
    "\n",
    "train_gen = DataGenerator(partition['train'], data, **params)\n",
    "val_gen = DataGenerator(partition['val'], data, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/danmoller/make-best-use-of-a-kernel-s-limited-uptime-keras\n",
    "import time \n",
    "\n",
    "#let's also import the abstract base class for our callback\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "#defining the callback\n",
    "class TimerCallback(Callback):\n",
    "    \n",
    "    def __init__(self, maxExecutionTime, byBatch = False, on_interrupt=None):\n",
    "        \n",
    "# Arguments:\n",
    "#     maxExecutionTime (number): Time in minutes. The model will keep training \n",
    "#                                until shortly before this limit\n",
    "#                                (If you need safety, provide a time with a certain tolerance)\n",
    "\n",
    "#     byBatch (boolean)     : If True, will try to interrupt training at the end of each batch\n",
    "#                             If False, will try to interrupt the model at the end of each epoch    \n",
    "#                            (use `byBatch = True` only if each epoch is going to take hours)          \n",
    "\n",
    "#     on_interrupt (method)          : called when training is interrupted\n",
    "#         signature: func(model,elapsedTime), where...\n",
    "#               model: the model being trained\n",
    "#               elapsedTime: the time passed since the beginning until interruption   \n",
    "\n",
    "        \n",
    "        self.maxExecutionTime = maxExecutionTime * 60\n",
    "        self.on_interrupt = on_interrupt\n",
    "        \n",
    "        #the same handler is used for checking each batch or each epoch\n",
    "        if byBatch == True:\n",
    "            #on_batch_end is called by keras every time a batch finishes\n",
    "            self.on_batch_end = self.on_end_handler\n",
    "        else:\n",
    "            #on_epoch_end is called by keras every time an epoch finishes\n",
    "            self.on_epoch_end = self.on_end_handler\n",
    "    \n",
    "    \n",
    "    #Keras will call this when training begins\n",
    "    def on_train_begin(self, logs):\n",
    "        self.startTime = time.time()\n",
    "        self.longestTime = 0            #time taken by the longest epoch or batch\n",
    "        self.lastTime = self.startTime  #time when the last trained epoch or batch was finished\n",
    "    \n",
    "    \n",
    "    #this is our custom handler that will be used in place of the keras methods:\n",
    "        #`on_batch_end(batch,logs)` or `on_epoch_end(epoch,logs)`\n",
    "    def on_end_handler(self, index, logs):\n",
    "        \n",
    "        currentTime      = time.time()                           \n",
    "        self.elapsedTime = currentTime - self.startTime    #total time taken until now\n",
    "        thisTime         = currentTime - self.lastTime     #time taken for the current epoch\n",
    "                                                               #or batch to finish\n",
    "        \n",
    "        self.lastTime = currentTime\n",
    "        \n",
    "        #verifications will be made based on the longest epoch or batch\n",
    "        if thisTime > self.longestTime:\n",
    "            self.longestTime = thisTime\n",
    "        \n",
    "        \n",
    "        #if the (assumed) time taken by the next epoch or batch is greater than the\n",
    "            #remaining time, stop training\n",
    "        remainingTime = self.maxExecutionTime - self.elapsedTime\n",
    "        if remainingTime < self.longestTime:\n",
    "            \n",
    "            self.model.stop_training = True  #this tells Keras to not continue training\n",
    "            print(\"\\n\\nTimerCallback: Finishing model training before it takes too much time. (Elapsed time: \" + str(self.elapsedTime/60.) + \" minutes )\\n\\n\")\n",
    "            \n",
    "            #if we have passed the `on_interrupt` callback, call it here\n",
    "            if self.on_interrupt is not None:\n",
    "                self.on_interrupt(self.model, self.elapsedTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4Fi-TjJ3FtZ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def f2_micro(y_true, y_pred):\n",
    "    agreement = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    total_true_positive = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    total_pred_positive = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    recall = agreement / (total_true_positive + K.epsilon())\n",
    "    precision = agreement / (total_pred_positive + K.epsilon())\n",
    "    return (1+2**2)*((precision*recall)/(2**2*precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "steps_per_epoch = partition['train'].shape[0]//BATCH_SIZE\n",
    "validation_steps = partition['val'].shape[0]//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint , ReduceLROnPlateau, History\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f2_micro(y_true, y_pred):\n",
    "    agreement = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    total_true_positive = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    total_pred_positive = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    recall = agreement / (total_true_positive + K.epsilon())\n",
    "    precision = agreement / (total_pred_positive + K.epsilon())\n",
    "    return (1+2**2)*((precision*recall)/(2**2*precision+recall+K.epsilon()))\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='linknet3.5.hdf5', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.001)\n",
    "\n",
    "hist = History()\n",
    "\n",
    "def default(obj):\n",
    "    if type(obj).__module__ == np.__name__:\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj.item()\n",
    "    raise TypeError('Unknown type:', type(obj))\n",
    "    \n",
    "def saveWeights(model, elapsed=None):\n",
    "    model.save('linknet3.5.hdf5')\n",
    "    json.dump(hist.history, open(\"hist_dict.json\", 'w'), default=default)\n",
    "    plt.plot(hist.history[\"loss\"],label=\"training loss\")\n",
    "    plt.plot(hist.history[\"val_loss\"],label=\"val loss\")\n",
    "    plt.title(\"LinkNet 3.5 Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "tc = TimerCallback(510, on_interrupt=saveWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gszNFU_K3lcw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/classification_models/resnext/__init__.py:4: UserWarning: Current ResNext models are deprecated, use keras.applications ResNeXt models\n",
      "  warnings.warn('Current ResNext models are deprecated, '\n",
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/type_check.py:546: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  'a.item() instead', DeprecationWarning, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(generator=<__main__...., validation_data=<__main__...., steps_per_epoch=8417, validation_steps=5795, epochs=4, callbacks=[<keras.ca..., verbose=1, use_multiprocessing=False)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "5419/8417 [==================>...........] - ETA: 26:57 - loss: 0.6456 - f2_micro: 0.3275"
     ]
    }
   ],
   "source": [
    "from segmentation_models import Linknet\n",
    "from segmentation_models.utils import set_trainable\n",
    "from segmentation_models.losses import dice_loss\n",
    "from keras_adabound import AdaBound\n",
    "\n",
    "model = Linknet(encoder_weights=None,encoder_freeze=False)\n",
    "model.compile(optimizer=AdaBound(lr=1e-3, final_lr=0.1), loss=dice_loss, metrics=[f2_micro])\n",
    "\n",
    "model.fit_generator(generator=train_gen,\n",
    "                    validation_data=val_gen,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    validation_steps=validation_steps,\n",
    "                    epochs=4,\n",
    "                    pickle_safe=False,\n",
    "                    callbacks=[checkpointer, reduce_lr, hist, tc],\n",
    "                    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Nl2l4cyy3Fta"
   ],
   "name": "yixin_airbus.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
